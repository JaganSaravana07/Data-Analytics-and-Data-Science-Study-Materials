{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###What is Exploratory Data Analysis (EDA)?\n",
        "Exploratory Data Analysis (EDA) is a crucial initial step in data science projects. It involves analyzing and visualizing data to understand its key characteristics, uncover patterns, and identify relationships between variables refers to the method of studying and exploring record sets to apprehend their predominant traits, discover patterns, locate outliers, and identify relationships between variables. EDA is normally carried out as a preliminary step before undertaking extra formal statistical analyses or modeling.\n",
        "\n",
        "###Key aspects of EDA include:\n",
        "\n",
        "- Distribution of Data: Examining the distribution of data points to understand their range, central tendencies (mean, median), and dispersion (variance, standard deviation).\n",
        "- Graphical Representations: Utilizing charts such as histograms, box plots, scatter plots, and bar charts to visualize relationships within the data and distributions of variables.\n",
        "- Outlier Detection: Identifying unusual values that deviate from other data points. Outliers can influence statistical analyses and might indicate data entry errors or unique cases.\n",
        "- Correlation Analysis: Checking the relationships between variables to understand how they might affect each other. This includes computing correlation coefficients and creating correlation matrices.\n",
        "- Handling Missing Values: Detecting and deciding how to address missing data points, whether by imputation or removal, depending on their impact and the amount of missing data.\n",
        "- Summary Statistics: Calculating key statistics that provide insight into data trends and nuances.\n",
        "- Testing Assumptions: Many statistical tests and models assume the data meet certain conditions (like normality or homoscedasticity). EDA helps verify these assumptions.\n",
        "\n",
        "###Why Exploratory Data Analysis is Important?\n",
        "Exploratory Data Analysis (EDA) is important for several reasons, especially in the context of data science and statistical modeling. Here are some of the key reasons why EDA is a critical step in the data analysis process:\n",
        "\n",
        "1. Understanding Data Structures: EDA helps in getting familiar with the dataset, understanding the number of features, the type of data in each feature, and the distribution of data points. This understanding is crucial for selecting appropriate analysis or prediction techniques.\n",
        "2. Identifying Patterns and Relationships: Through visualizations and statistical summaries, EDA can reveal hidden patterns and intrinsic relationships between variables. These insights can guide further analysis and enable more effective feature engineering and model building.\n",
        "3. Detecting Anomalies and Outliers: EDA is essential for identifying errors or unusual data points that may adversely affect the results of your analysis. Detecting these early can prevent costly mistakes in predictive modeling and analysis.\n",
        "4. Testing Assumptions: Many statistical models assume that data follow a certain distribution or that variables are independent. EDA involves checking these assumptions. If the assumptions do not hold, the conclusions drawn from the model could be invalid.\n",
        "5. Informing Feature Selection and Engineering: Insights gained from EDA can inform which features are most relevant to include in a model and how to transform them (scaling, encoding) to improve model performance.\n",
        "6. Optimizing Model Design: By understanding the data’s characteristics, analysts can choose appropriate modeling techniques, decide on the complexity of the model, and better tune model parameters.\n",
        "7. Facilitating Data Cleaning: EDA helps in spotting missing values and errors in the data, which are critical to address before further analysis to improve data quality and integrity.\n",
        "8. Enhancing Communication: Visual and statistical summaries from EDA can make it easier to communicate findings and convince others of the validity of your conclusions, particularly when explaining data-driven insights to stakeholders without technical backgrounds.\n",
        "\n",
        "###Types of Exploratory Data Analysis\n",
        "EDA, or Exploratory Data Analysis, refers back to the method of analyzing and analyzing information units to uncover styles, pick out relationships, and gain insights. There are various sorts of EDA strategies that can be hired relying on the nature of the records and the desires of the evaluation. Depending on the number of columns we are analyzing we can divide EDA into three types: `Univariate, bivariate and multivariate`.\n",
        "\n",
        "1. Univariate Analysis\n",
        "\n",
        "   Univariate analysis focuses on a single variable to understand its internal structure. It is primarily concerned with describing the data and finding patterns existing in a single feature. This sort of evaluation makes a speciality of analyzing character variables inside the records set. It involves summarizing and visualizing a unmarried variable at a time to understand its distribution, relevant tendency, unfold, and different applicable records. Common techniques include:\n",
        "\n",
        "- Histograms: Used to visualize the distribution of a variable.\n",
        "- Box plots: Useful for detecting outliers and understanding the spread and skewness of the data.\n",
        "- Bar charts: Employed for categorical data to show the frequency of each category.\n",
        "- Summary statistics: Calculations like mean, median, mode, variance, and standard deviation that describe the central tendency and dispersion of the data.\n",
        "\n",
        "2. Bivariate Analysis\n",
        "\n",
        "   Bivariate evaluation involves exploring the connection between variables. It enables find associations, correlations, and dependencies between pairs of variables. Bivariate analysis is a crucial form of exploratory data analysis that examines the relationship between two variables. Some key techniques used in bivariate analysis:\n",
        "\n",
        "- Scatter Plots: These are one of the most common tools used in bivariate analysis. A scatter plot helps visualize the relationship between two continuous variables.\n",
        "- Correlation Coefficient: This statistical measure (often Pearson’s correlation coefficient for linear relationships) quantifies the degree to which two variables are related.\n",
        "- Cross-tabulation: Also known as contingency tables, cross-tabulation is used to analyze the relationship between two categorical variables. It shows the frequency distribution of categories of one variable in rows and the other in columns, which helps in understanding the relationship between the two variables.\n",
        "- Line Graphs: In the context of time series data, line graphs can be used to compare two variables over time. This helps in identifying trends, cycles, or patterns that emerge in the interaction of the variables over the specified period.\n",
        "- Covariance: Covariance is a measure used to determine how much two random variables change together. However, it is sensitive to the scale of the variables, so it’s often supplemented by the correlation coefficient for a more standardized assessment of the relationship.\n",
        "\n",
        "3. Multivariate Analysis\n",
        "\n",
        "   Multivariate analysis examines the relationships between two or more variables in the dataset. It aims to understand how variables interact with one another, which is crucial for most statistical modeling techniques. Techniques include:\n",
        "\n",
        "- Pair plots: Visualize relationships across several variables simultaneously to capture a comprehensive view of potential interactions.\n",
        "- Principal Component Analysis (PCA): A dimensionality reduction technique used to reduce the dimensionality of large datasets, while preserving as much variance as possible.\n",
        "\n",
        "####Specialized EDA Techniques\n",
        "In addition to univariate and multivariate analysis, there are specialized EDA techniques tailored for specific types of data or analysis needs:\n",
        "\n",
        "- Spatial Analysis: For geographical data, using maps and spatial plotting to understand the geographical distribution of variables.\n",
        "- Text Analysis: Involves techniques like word clouds, frequency distributions, and sentiment analysis to explore text data.\n",
        "- Time Series Analysis: This type of analysis is mainly applied to statistics sets that have a temporal component. Time collection evaluation entails inspecting and modeling styles, traits, and seasonality inside the statistics through the years. Techniques like line plots, autocorrelation analysis, transferring averages, and ARIMA (AutoRegressive Integrated Moving Average) fashions are generally utilized in time series analysis.\n",
        "\n",
        "###Steps for Performing Exploratory Data Analysis\n",
        "####Step 1: Understand the Problem and the Data\n",
        "The first step in any information evaluation project is to sincerely apprehend the trouble you are trying to resolve and the statistics you have at your disposal. This entails asking questions consisting of:\n",
        "\n",
        "- What is the commercial enterprise goal or research question you are trying to address?\n",
        "- What are the variables inside the information, and what do they mean?\n",
        "- What are the data sorts (numerical, categorical, textual content, etc.) ?\n",
        "- Is there any known information on first-class troubles or obstacles?\n",
        "- Are there any relevant area-unique issues or constraints?\n",
        "\n",
        "By thoroughly knowing the problem and the information, you can better formulate your evaluation technique and avoid making incorrect assumptions or drawing misguided conclusions. It is also vital to contain situations and remember specialists or stakeholders to this degree to ensure you have complete know-how of the context and requirements.\n",
        "\n",
        "####Step 2: Import and Inspect the Data\n",
        "Once you have clean expertise of the problem and the information, the following step is to import the data into your evaluation environment (e.g., Python, R, or a spreadsheet program). During this step, looking into the statistics is critical to gain initial know-how of its structure, variable kinds, and capability issues.\n",
        "\n",
        "Here are a few obligations you could carry out at this stage:\n",
        "\n",
        "- Load the facts into your analysis environment, ensuring that the facts are imported efficiently and without errors or truncations.\n",
        "- Examine the size of the facts (variety of rows and columns) to experience its length and complexity.\n",
        "- Check for missing values and their distribution across variables, as missing information can notably affect the quality and reliability of your evaluation.\n",
        "- Identify facts sorts and formats for each variable, as these records may be necessary for the following facts manipulation and evaluation steps.\n",
        "- Look for any apparent errors or inconsistencies in the information, such as invalid values, mismatched units, or outliers, that can indicate exceptional issues with information.\n",
        "\n",
        "####Step 3: Handle Missing Data\n",
        "Missing records is a joint project in many datasets, and it can significantly impact the quality and reliability of your evaluation. During the EDA method, it’s critical to pick out and deal with lacking information as it should be, as ignoring or mishandling lacking data can result in biased or misleading outcomes.\n",
        "\n",
        "Here are some techniques you could use to handle missing statistics:\n",
        "\n",
        "- Understand the styles and capacity reasons for missing statistics: Is the information lacking entirely at random (MCAR), lacking at random (MAR), or lacking not at random (MNAR)? Understanding the underlying mechanisms can inform the proper method for handling missing information.\n",
        "- Decide whether to eliminate observations with lacking values (listwise deletion) or attribute (fill in) missing values: Removing observations with missing values can result in a loss of statistics and potentially biased outcomes, specifically if the lacking statistics are not MCAR. Imputing missing values can assist in preserving treasured facts. However, the imputation approach needs to be chosen cautiously.\n",
        "- Use suitable imputation strategies, such as mean/median imputation, regression imputation, a couple of imputations, or device-getting-to-know-based imputation methods like k-nearest associates (KNN) or selection trees. The preference for the imputation technique has to be primarily based on the characteristics of the information and the assumptions underlying every method.\n",
        "- Consider the effect of lacking information: Even after imputation, lacking facts can introduce uncertainty and bias. It is important to acknowledge those limitations and interpret your outcomes with warning.\n",
        "\n",
        "Handling missing information nicely can improve the accuracy and reliability of your evaluation and save you biased or deceptive conclusions. It is likewise vital to record the techniques used to address missing facts and the motive in the back of your selections.\n",
        "\n",
        "####Step 4: Explore Data Characteristics\n",
        "After addressing the facts that are lacking, the next step within the EDA technique is to explore the traits of your statistics. This entails examining your variables’ distribution, crucial tendency, and variability and identifying any ability outliers or anomalies. Understanding the characteristics of your information is critical in deciding on appropriate analytical techniques, figuring out capability information first-rate troubles, and gaining insights that may tell subsequent evaluation and modeling decisions.\n",
        "\n",
        "Calculate summary facts (suggest, median, mode, preferred deviation, skewness, kurtosis, and many others.) for numerical variables: These facts provide a concise assessment of the distribution and critical tendency of each variable, aiding in the identification of ability issues or deviations from expected patterns.\n",
        "\n",
        "####Step 5: Perform Data Transformation\n",
        "Data transformation is a critical step within the EDA process because it enables you to prepare your statistics for similar evaluation and modeling. Depending on the traits of your information and the necessities of your analysis, you may need to carry out various ameliorations to ensure that your records are in the most appropriate layout.\n",
        "\n",
        "Here are a few common records transformation strategies:\n",
        "\n",
        "- Scaling or normalizing numerical variables to a standard variety (e.g., min-max scaling, standardization)\n",
        "- Encoding categorical variables to be used in machine mastering fashions (e.g., one-warm encoding, label encoding)\n",
        "- Applying mathematical differences to numerical variables (e.g., logarithmic, square root) to correct for skewness or non-linearity\n",
        "- Creating derived variables or capabilities primarily based on current variables (e.g., calculating ratios, combining variables)\n",
        "- Aggregating or grouping records mainly based on unique variables or situations\n",
        "\n",
        "By accurately transforming your information, you could ensure that your evaluation and modeling strategies are implemented successfully and that your results are reliable and meaningful.\n",
        "\n",
        "####Step 6: Visualize Data Relationships\n",
        "Visualization is an effective tool in the EDA manner, as it allows to discover relationships between variables and become aware of styles or trends that may not immediately be apparent from summary statistics or numerical outputs. To visualize data relationships, explore univariate, bivariate, and multivariate analysis.\n",
        "\n",
        "- Create frequency tables, bar plots, and pie charts for express variables: These visualizations can help you apprehend the distribution of classes and discover any ability imbalances or unusual patterns.\n",
        "- Generate histograms, container plots, violin plots, and density plots to visualize the distribution of numerical variables. These visualizations can screen critical information about the form, unfold, and ability outliers within the statistics.\n",
        "- Examine the correlation or association among variables using scatter plots, correlation matrices, or statistical assessments like Pearson’s correlation coefficient or Spearman’s rank correlation: Understanding the relationships between variables can tell characteristic choice, dimensionality discount, and modeling choices.\n",
        "\n",
        "####Step 7: Handling Outliers\n",
        "An Outlier is a data item/object that deviates significantly from the rest of the (so-called normal)objects. They can be caused by measurement or execution errors. The analysis for outlier detection is referred to as outlier mining. There are many ways to detect outliers, and the removal process of these outliers from the dataframe is the same as removing a data item from the panda’s dataframe.\n",
        "\n",
        "Identify and inspect capability outliers through the usage of strategies like the interquartile range (IQR), Z-scores, or area-specific regulations: Outliers can considerably impact the results of statistical analyses and gadget studying fashions, so it’s essential to perceive and take care of them as it should be.\n",
        "\n",
        "####Step 8: Communicate Findings and Insights\n",
        "The final step in the EDA technique is effectively discussing your findings and insights. This includes summarizing your evaluation, highlighting fundamental discoveries, and imparting your outcomes cleanly and compellingly.\n",
        "\n",
        "Here are a few hints for effective verbal exchange:\n",
        "\n",
        "- Clearly state the targets and scope of your analysis\n",
        "- Provide context and heritage data to assist others in apprehending your approach\n",
        "- Use visualizations and photos to guide your findings and make them more reachable\n",
        "- Highlight critical insights, patterns, or anomalies discovered for the duration of the EDA manner\n",
        "- Discuss any barriers or caveats related to your analysis\n",
        "- Suggest ability next steps or areas for additional investigation\n",
        "\n",
        "Effective conversation is critical for ensuring that your EDA efforts have a meaningful impact and that your insights are understood and acted upon with the aid of stakeholders.\n",
        "\n",
        "###Conclusion\n",
        "Exploratory Data Analysis forms the bedrock of data science endeavors, offering invaluable insights into dataset nuances and paving the path for informed decision-making. By delving into data distributions, relationships, and anomalies, EDA empowers data scientists to unravel hidden truths and steer projects toward success.\n",
        "\n"
      ],
      "metadata": {
        "id": "INLAIOiF4NAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Understand the Problem and the Data\n"
      ],
      "metadata": {
        "id": "OVJFqEWiBudf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K92oz0Wu3_-P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# read datasdet using pandas\n",
        "df = pd.read_csv('employees.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Import and Inspect the Data"
      ],
      "metadata": {
        "id": "3_9dPlDACDpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "d9ODbRPLCH2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# information about the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "BPCzWgfjCRS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "s57l5tglCTky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "296mmLviCVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Handling Missing Values\n",
        "You all must be wondering why a dataset will contain any missing values. It can occur when no information is provided for one or more items or for a whole unit. For Example, Suppose different users being surveyed may choose not to share their income, and some users may choose not to share their address in this way many datasets went missing. Missing Data is a very big problem in real-life scenarios.\n",
        "\n",
        "Missing Data can also refer to as NA(Not Available) values in pandas. There are several useful functions for detecting, removing, and replacing null values in Pandas DataFrame :\n",
        "\n",
        "- `isnull()`\n",
        "- `notnull()`\n",
        "- `dropna()`\n",
        "- `fillna()`\n",
        "- `replace()`\n",
        "- `interpolate()`"
      ],
      "metadata": {
        "id": "jxow5bCbCPiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "6y6EpPtACl_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4: Explore Data Characteristics\n",
        "By exploring the characteristics of your information very well, you can gain treasured insights into its structure, pick out capability problems or anomalies, and inform your subsequent evaluation and modeling choices. Documenting any findings or observations from this step is critical, as they may be relevant for destiny reference or communication with stakeholders.\n",
        "\n",
        "Let’s start by exploring the data according to the dataset. We’ll begin with Gender Diversity Analysis by looking at:\n",
        "\n",
        "- Gender distribution across the company.\n",
        "- Departments or teams with significant gender imbalances.\n",
        "\n",
        "####Gender Distribution Across the Company\n",
        "\n",
        "We’ll calculate the proportion of each gender across the company. ​\n",
        "\n",
        "Start Date is an important column for employees. However, it is not of much use if we can not handle it properly. To handle this type of data pandas provide a special function from which we can change object type to DateTime format datetime()."
      ],
      "metadata": {
        "id": "2__mJGx-CPgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Start Date' to datetime format\n",
        "df['Start Date'] = pd.to_datetime(df['Start Date'])\n",
        "\n",
        "# Convert 'Last Login Time' to time format\n",
        "df['Last Login Time'] = pd.to_datetime(df['Last Login Time']).dt.time\n",
        "df.dtypes, df.head()"
      ],
      "metadata": {
        "id": "Sy34_sbrDJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gender distribution across the company\n",
        "gender_distribution = df['Gender'].value_counts(normalize=True) * 100\n",
        "gender_distribution"
      ],
      "metadata": {
        "id": "1hpv9126DSnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5: Perform Data Transformation\n",
        "Data transformation is a critical step within the EDA process because it enables you to prepare your statistics for similar evaluation and modeling. Depending on the traits of your information and the necessities of your analysis, you may need to carry out various ameliorations to ensure that your records are in the most appropriate layout.\n",
        "\n",
        "Here are a few common records transformation strategies:\n",
        "\n",
        "- Scaling or normalizing numerical variables to a standard variety (e.g., min-max scaling, standardization)\n",
        "- Encoding categorical variables to be used in machine mastering fashions (e.g., one-warm encoding, label encoding)\n",
        "- Applying mathematical differences to numerical variables (e.g., logarithmic, square root) to correct for skewness or non-linearity\n",
        "- Creating derived variables or capabilities primarily based on current variables (e.g., calculating ratios, combining variables)\n",
        "- Aggregating or grouping records mainly based on unique variables or situations\n",
        "\n",
        "By accurately transforming your information, you could ensure that your evaluation and modeling strategies are implemented successfully and that your results are reliable and meaningful.\n",
        "\n",
        "####Encoding Categorical Variables\n",
        "There are some models like `Linear Regression` which does not work with categorical dataset in that case we should try to encode categorical dataset into the numerical column. We can use different methods for encoding like Label encoding or `One-hot encoding`. pandas and sklearn provide different functions for encoding in our case we will use the `LabelEncoding` function from sklearn to encode the Gender column."
      ],
      "metadata": {
        "id": "n6Ibn400DcaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "# fit and transform the \"Senior Management\" column with LabelEncoder\n",
        "df['Gender'] = le.fit_transform(df['Gender'])"
      ],
      "metadata": {
        "id": "vMkF6UxfDyNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 6: Visualize Data Relationships\n",
        "To visualize data relationships, we’ll explore univariate, bivariate, and multivariate analyses using the employees dataset. These visualizations will help uncover patterns, trends, and relationships within the data.\n",
        "\n",
        "####Univariate Analysis\n",
        "This analysis focuses on a single variable. Here, we’ll look at the distributions of ‘Salary’ and ‘Bonus %’.\n",
        "\n",
        "1. Histogram of Salary\n",
        "2. Histogram of Bonus %\n",
        "\n",
        "Histograms and density plots are typically used to visualize the distribution. These plots can show the spread, central tendency, and any skewness in the data."
      ],
      "metadata": {
        "id": "jJxn5YW1D0o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Analysis: Histograms for 'Salary' and 'Bonus %'\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "sns.histplot(df['Salary'], bins=30, kde=True, ax=axes[0])\n",
        "axes[0].set_title('Histogram of Salary')\n",
        "\n",
        "sns.histplot(df['Bonus %'], bins=30, kde=True, ax=axes[1])\n",
        "axes[1].set_title('Histogram of Bonus %')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C_9WDnMTD2m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bivariate Analysis\n",
        "Bivariate analysis explores the relationship between two variables. Common visualizations include Scatter Plot and Box Plots."
      ],
      "metadata": {
        "id": "82uJkJSdD_no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Boxplot For Data Visualization"
      ],
      "metadata": {
        "id": "kgJkUxSyEHTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "sns.boxplot( x=\"Salary\", y='Team', data=df, )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wC4QIchWEDlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Scatter Plot For Data Visualization"
      ],
      "metadata": {
        "id": "TOrCLQSkEJaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "sns.scatterplot( x=\"Salary\", y='Team', data=df,\n",
        "                hue='Gender', size='Bonus %')\n",
        "\n",
        "# Placing Legend outside the Figure\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wjzzbhMvEKd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multivariate Analysis\n",
        "Multivariate analysis involves examining the relationships among three or more variables. Some common methods include:\n",
        "\n",
        "1. Pair Plots: To visualize pairwise relationships across several variables at once.\n",
        "2. Heatmaps: Particularly useful for showing the correlation matrix between numerical variables.\n",
        "3. Faceted Grids: Allow you to explore data across many dimensions and are particularly useful for understanding the interaction effects among variables.\n",
        "\n",
        "For Now, we will use pairplot()method of the seaborn module. We can also use it for the multiple pairwise bivariate distributions in a dataset."
      ],
      "metadata": {
        "id": "i5DSde1nEMO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "sns.pairplot(df, hue='Gender', height=2)"
      ],
      "metadata": {
        "id": "OrD4pVEcETMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 7: Handling Outliers\n",
        "\n",
        "An Outlier is a data item/object that deviates significantly from the rest of the (so-called normal)objects. They can be caused by measurement or execution errors. The analysis for outlier detection is referred to as outlier mining. There are many ways to detect outliers, and the removal process of these outliers from the dataframe is the same as removing a data item from the panda’s dataframe.\n",
        "\n",
        "To handle outliers effectively, we need to identify them in key numerical variables that could significantly impact our analysis. For this dataset,\n",
        "####we’ll focus on ‘Salary’ and ‘Bonus %’ as these are critical financial metrics.\n",
        "\n",
        "We’ll use the Interquartile Range (IQR) method to identify outliers in these variables. The IQR method is robust as it defines outliers based on the statistical spread of the data."
      ],
      "metadata": {
        "id": "5UKco4lfgQdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate IQR for Salary and Bonus %\n",
        "Q1_salary = df['Salary'].quantile(0.25)\n",
        "Q3_salary = df['Salary'].quantile(0.75)\n",
        "IQR_salary = Q3_salary - Q1_salary\n",
        "\n",
        "Q1_bonus = df['Bonus %'].quantile(0.25)\n",
        "Q3_bonus = df['Bonus %'].quantile(0.75)\n",
        "IQR_bonus = Q3_bonus - Q1_bonus\n",
        "\n",
        "# Define outliers\n",
        "outliers_salary = df[(df['Salary'] < (Q1_salary - 1.5 * IQR_salary)) |\n",
        "                                 (df['Salary'] > (Q3_salary + 1.5 * IQR_salary))]\n",
        "\n",
        "outliers_bonus = df[(df['Bonus %'] < (Q1_bonus - 1.5 * IQR_bonus)) |\n",
        "                                (df['Bonus %'] > (Q3_bonus + 1.5 * IQR_bonus))]\n",
        "\n",
        "# Plotting boxplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "sns.boxplot(x=df['Salary'], ax=axes[0])\n",
        "axes[0].set_title('Boxplot of Salary')\n",
        "sns.boxplot(x=df['Bonus %'], ax=axes[1])\n",
        "axes[1].set_title('Boxplot of Bonus %')\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n",
        "\n",
        "# Display the number of outliers detected\n",
        "outliers_salary.shape[0], outliers_bonus.shape[0]"
      ],
      "metadata": {
        "id": "1R93ilNognEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For removing the outlier, one must follow the same process of removing an entry from the dataset using its exact position in the dataset because in all the above methods of detecting the outliers end result is the list of all those data items that satisfy the outlier definition according to the method used."
      ],
      "metadata": {
        "id": "I8G-D7yLgrzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 8: Communicate Findings and Insights\n",
        "The final step in the EDA technique is effectively discussing your findings and insights. This includes summarizing your evaluation, highlighting fundamental discoveries, and imparting your outcomes cleanly and compellingly.\n",
        "\n",
        "Here are a few hints for effective verbal exchange:\n",
        "\n",
        "- Clearly state the targets and scope of your analysis\n",
        "- Provide context and heritage data to assist others in apprehending your approach\n",
        "- Use visualizations and photos to guide your findings and make them more reachable\n",
        "- Highlight critical insights, patterns, or anomalies discovered for the duration of the EDA manner\n",
        "- Discuss any barriers or caveats related to your analysis\n",
        "- Suggest ability next steps or areas for additional investigation\n",
        "\n",
        "Effective conversation is critical for ensuring that your EDA efforts have a meaningful impact and that your insights are understood and acted upon with the aid of stakeholders.\n",
        "\n",
        "###Conclusion\n",
        "Exploratory Data Analysis is a powerful and vital technique for gaining deep information about your records earlier than venture formal modeling or speculation testing. By following the seven steps mentioned in this newsletter – knowing how the problem and information, uploading and inspecting the information, managing missing information, exploring data traits, appearing data transformation, visualizing data relationships, and communicating findings and insights – you may free up the whole potential of your records and extract valuable insights that could pressure informed decision-making.\n",
        "\n",
        "Mastering EDA requires technical skills, analytical wandering, and powerful communique talents. As you exercise and refine your EDA abilities, you become more ready to tackle complicated facts and demanding situations and uncover insights that can offer an aggressive edge for your agency."
      ],
      "metadata": {
        "id": "OhUyS1BvguKN"
      }
    }
  ]
}