{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "Pre-processing refers to the transformations applied to our data before feeding it to the algorithm. Data preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
        "\n",
        "##Need of Data Preprocessing\n",
        "For achieving better results from the applied model in Machine Learning projects the format of the data has to be in a proper manner. Some specified Machine Learning model needs information in a specified format, for example, Random Forest algorithm does not support null values, therefore to execute random forest algorithm null values have to be managed from the original raw data set.\n",
        "Another aspect is that the data set should be formatted in such a way that more than one Machine Learning and Deep Learning algorithm are executed in one data set, and best out of them is chosen.\n",
        "\n",
        "##Example Dataset\n",
        "We'll continue using the fictional dataset customers.csv with the following columns:\n",
        "\n",
        "- CustomerID: Unique identifier for each customer.\n",
        "- Age: Age of the customer.\n",
        "- Income: Annual income of the customer.\n",
        "- Gender: Gender of the customer (Male/Female).\n",
        "- SpendingScore: A score assigned by the retail store based on customer behavior and spending patterns (scale of 1-100).\n",
        "- City: The city where the customer resides.\n",
        "- JoinedDate: The date when the customer joined the store's loyalty program.\n",
        "\n",
        "###Steps in Data Preprocessing\n",
        "Step 1: Import the necessary libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oXpiY6yeqgZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf63zfYml3CN"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load the dataset\n",
        "\n",
        "Explanation: Load data into a pandas DataFrame."
      ],
      "metadata": {
        "id": "fSWowxqHq0KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('customers.csv')"
      ],
      "metadata": {
        "id": "1adT2f6Qq1s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Understanding the Dataset\n",
        "\n",
        "Explanation: Explore the dataset's structure and summary statistics.\n"
      ],
      "metadata": {
        "id": "SZ8JlX78ruyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "cHa4Biv3sA4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the null values using `df.isnull()`"
      ],
      "metadata": {
        "id": "v46UcqkcsJsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "dODcw_FosBeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Statistical Analysis\n",
        "\n",
        "In statistical analysis, first, we use the `df.describe()` which will give a descriptive overview of the dataset."
      ],
      "metadata": {
        "id": "9g9FPpF5sPoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "id": "N37dt48esBcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Handling Missing Data\n",
        "\n",
        "Explanation: Handle missing values by filling or removing them."
      ],
      "metadata": {
        "id": "BLBFo_cTtiB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Income'].fillna(df['Income'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "9C9HLMiysBYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Handling Outliers\n",
        "\n",
        "###What is Outlier?\n",
        "\n",
        "An Outlier is a data item/object that deviates significantly from the rest of the (so-called normal) objects. Identifying outliers is important in statistics and data analysis because they can have a significant impact on the results of statistical analyses. The analysis for outlier detection is referred to as outlier mining.\n",
        "\n",
        "Outliers can skew the mean (average) and affect measures of central tendency, as well as influence the results of tests of statistical significance.\n",
        "\n",
        "###How Outliers are Caused?\n",
        "\n",
        "Outliers can be caused by a variety of factors, and they often result from genuine variability in the data or from errors in data collection, measurement, or recording. Some common causes of outliers are:\n",
        "\n",
        "- Measurement errors: Errors in data collection or measurement processes can lead to outliers.\n",
        "- Sampling errors: In some cases, outliers can arise due to issues with the sampling process.\n",
        "- Natural variability: Inherent variability in certain phenomena can also lead to outliers. Some systems may exhibit extreme values due to the nature of the process being studied.\n",
        "- Data entry errors: Human errors during data entry can introduce outliers.\n",
        "Experimental errors: In experimental settings, anomalies may occur due to uncontrolled factors, equipment malfunctions, or unexpected events.\n",
        "- Sampling from multiple populations: Data is inadvertently combined from multiple populations with different characteristics.\n",
        "- Intentional outliers: Outliers are introduced intentionally to test the robustness of statistical methods.\n",
        "\n",
        "###Outlier Detection And Removal\n",
        "\n",
        "Here pandas data frame is used for a more realistic approach as real-world projects need to detect the outliers that arose during the data analysis step, the same approach can be used on lists and series-type objects.\n",
        "\n",
        "For example, if most customers have an annual income between $30,000 and $100,000, a customer with an income of $1,000,000 might be considered an outlier.\n",
        "\n",
        "Explanation: Identify and remove outliers to prevent them from skewing the analysis.\n",
        "\n",
        "###Detecting Outliers Using the Interquartile Range (IQR) Method\n",
        "\n",
        "The IQR method is a common technique for detecting outliers:\n",
        "\n",
        "- IQR: The range between the first quartile (25th percentile) and the third quartile (75th percentile).\n",
        "\n",
        "- Outliers: Data points below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR."
      ],
      "metadata": {
        "id": "sfTlDGjctrkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = df['Income'].quantile(0.25)\n",
        "Q3 = df['Income'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Detecting outliers\n",
        "outliers = df[(df['Income'] < lower_bound) | (df['Income'] > upper_bound)]\n",
        "print(\"Detected outliers:\")\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "d--pBTAwsBWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualizing Outliers Using Boxplot\n",
        "Visualizing outliers can help in understanding their distribution. A boxplot is a great way to visualize the presence of outliers."
      ],
      "metadata": {
        "id": "NM7sP6q1vmbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot to visualize outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "df.boxplot(column=['Income'])\n",
        "plt.title('Boxplot of Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aKrd9ZVhsBT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot displays the distribution of the Income variable. The points outside the whiskers of the boxplot are considered outliers."
      ],
      "metadata": {
        "id": "OHRDidQtv5Lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Removing Outliers\n",
        "Once the outliers are detected, they can be removed to prevent them from skewing the analysis."
      ],
      "metadata": {
        "id": "4giL35LUvwAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers\n",
        "df_cleaned = df[(df['Income'] >= lower_bound) & (df['Income'] <= upper_bound)]\n",
        "print(\"Data after removing outliers:\")\n",
        "print(df_cleaned.head())"
      ],
      "metadata": {
        "id": "Zg8FXRpvsBR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cleaned dataset df_cleaned now contains only those data points that lie within the calculated bounds, effectively removing the detected outliers."
      ],
      "metadata": {
        "id": "t8IFL7Bcv29q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Verifying Outliers Removal\n",
        "After removing outliers, it’s a good practice to verify that they have been successfully removed."
      ],
      "metadata": {
        "id": "qkvfD4bZv87z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if any outliers are still present\n",
        "outliers_after_removal = df_cleaned[(df_cleaned['Income'] < lower_bound) | (df_cleaned['Income'] > upper_bound)]\n",
        "print(\"Outliers present after removal:\")\n",
        "print(outliers_after_removal)"
      ],
      "metadata": {
        "id": "XO_XlkAdsBPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step ensures that all outliers have been removed and the dataset is now clean."
      ],
      "metadata": {
        "id": "C-JF1B8CwBbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Cleaned Data\n",
        "You can visualize the cleaned data using a boxplot to confirm that the outliers have been removed."
      ],
      "metadata": {
        "id": "AFWCPhMmwDdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot after removing outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_cleaned.boxplot(column=['Income'])\n",
        "plt.title('Boxplot of Income After Outlier Removal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QShGCACesBNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot should no longer show any points outside the whiskers, indicating that the outliers have been successfully removed."
      ],
      "metadata": {
        "id": "O4hmdDu8wKcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoding Categorical Data\n",
        "Explanation: Convert categorical variables into numerical format."
      ],
      "metadata": {
        "id": "k63GqDsrwWgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=['City'])\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Gender'] = le.fit_transform(df['Gender'])"
      ],
      "metadata": {
        "id": "Iw2yb5hHwLAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Scaling\n",
        "a. Standardization\n",
        "- Explanation: Scale features to have a mean of 0 and standard deviation of 1.\n",
        "- When to Use: Use when features have different units or distributions that need to be normalized."
      ],
      "metadata": {
        "id": "PfdODCgdwd0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])"
      ],
      "metadata": {
        "id": "gvRsDrzwwcW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Normalization\n",
        "- Explanation: Scale features to a range (usually 0 to 1).\n",
        "- When to Use: Use when features need to be scaled to a specific range, often in algorithms sensitive to scales like Neural Networks."
      ],
      "metadata": {
        "id": "Cx3Bt6zBwoJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[['SpendingScore']] = scaler.fit_transform(df[['SpendingScore']])"
      ],
      "metadata": {
        "id": "guYzUoj3wcVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Correlation Analysis\n",
        "- Explanation: Check the correlation between features to understand their relationships.\n",
        "- When to Use: Use to identify multicollinearity or redundant features."
      ],
      "metadata": {
        "id": "eSQuBxWGwvza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "pkEKG_t3wcTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation matrix shows the strength and direction of relationships between features. For example, if Income and SpendingScore have a high positive correlation, it might indicate that wealthier customers spend more."
      ],
      "metadata": {
        "id": "Ua5pbc9Ew5wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Engineering\n",
        "Explanation: Create or transform features to reveal patterns."
      ],
      "metadata": {
        "id": "cAHwf2y3w7L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 60, 100], labels=['Youth', 'Adult', 'Senior', 'Elderly'])\n"
      ],
      "metadata": {
        "id": "N9NweuiOwcQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dimensionality Reduction\n",
        "Explanation: Reduce the number of features while retaining most of the variance."
      ],
      "metadata": {
        "id": "vZok_-faw9cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df)\n"
      ],
      "metadata": {
        "id": "3_BesxUMwcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Splitting the Dataset\n",
        "Explanation: Split the data into training and testing sets for model evaluation."
      ],
      "metadata": {
        "id": "f9NBjn3NxCSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop('SpendingScore', axis=1)\n",
        "y = df['SpendingScore']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "pDTnx829xE25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dealing with Imbalanced Data\n",
        "Explanation: Balance classes in classification problems using techniques like SMOTE."
      ],
      "metadata": {
        "id": "d5WEjEevxKS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "oWTv2uGtxNXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Data Preprocessing (for NLP)\n",
        "Explanation: Preprocess text data by tokenizing, removing stopwords, and stemming."
      ],
      "metadata": {
        "id": "TEvrh3LAxSL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "df['tokens'] = df['CustomerFeedback'].apply(word_tokenize)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "ps = PorterStemmer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [ps.stem(word) for word in x])\n"
      ],
      "metadata": {
        "id": "Zy2Egdu1xO9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving and Loading Preprocessed Data"
      ],
      "metadata": {
        "id": "-fbELYPVxVhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('preprocessed_customers.csv', index=False)\n",
        "df = pd.read_csv('preprocessed_customers.csv')"
      ],
      "metadata": {
        "id": "DZhKwVn_xO6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}